# Custom Environment Guide

This document presents how to deploy ERFSL to your custom MORL environment, and combine ERFSL with human prior knowledge.

## Files to be Replaced/Modified

The `prompt` folder contains both task-related and task-independent prompts. To use these in your custom environment, you need to modify the task-related prompts. The following is a brief guide on deploying the ERFSL framework into your MORL environment.

1. As noted in the paper, you should write a task description similar to our example, including:

- A short text that briefly describes the environment, and the necessary Python code, including variables and functions for the reward function design, or APIs.
- Numerical user requirements of explicit numerical objectives without coupling. If certain requirements cannot be decoupled, combine them into a single requirement.

The prompt for these elements should be written to `prompts/task_relative/env_description.py`.

Additionally, to facilitate the reward critic's alignment of requirements with reward components, you should also write a more detailed requirements description and express it as a Python `dict`, stored in `prompts/task_relative_task_objective.py`. Specifically, this should include a short phrase for each requirement (unit for reward component), along with the corresponding description. For example: `{'collision':'(One of safety requirement) The number of both **collisions** should be **reduced to zero**.'}`

Optionally, you can provide guidance on designing reward functions if necessary, such as some example code or problems you might want LLMs to avoid, etc.

Although we provide a meta-prompt to critique the description, we recommend confirming a simple checklist:

> Make sure your task description is understandable to someone without any experience.
>
> Make sure you have not omitted any necessary user requirements and a description of the LLM.
>
> Make sure your description is logically organized and well structured.

2. Run this command to utilize the meta-prompt, which provides suggestions for the environment description specified in the file `prompts/task_relative/env_description.py`. To prevent hallucinations or errors from LLMs directly modifying the prompt, you must manually adjust the description prompt as suggested:

```bash
python check_meta_prompt.py
```

3. Modify or replace these task-relative Python code files:

> `env.py`：Your RL environment (or environment entry point) Python file.
>
> `train_ddpg.py`：Replace it with your RL training Python script.
>
> `run_train.py`：You may modify line 17 to specify the entry point of training: 
>
> ```python
> args = ['python','train_td3.py','--iter',str(args.iter),'--reward_no',str(i+1),
>          '--episode_num',str(ep_num)] + unknown # train_td3.py -> your RL training script 
> ```
>
> Additionally, if your RL training script has additional arguments to specify, you can list them in the `arguments` variable of `config.py`.

To interface with ERFSL, note the following: 

1. Dynamically import and execute the reward function generated by LLM. Here is a simple example, see lines 67-73 and 208 in `env.py`:

```python
# import
import importlib.util
# line 67 - 73
		try:
            self.spec_r = importlib.util.spec_from_file_location("r",os.getcwd() + f'/reward_funcs/reward_ITER{self.iter}_REWARD{self.reward_no}.py')
            self.module_r = importlib.util.module_from_spec(self.spec_r)
            self.spec_r.loader.exec_module(self.module_r)
            self.reward_ok = True
        except:
            print("The reward function files can not be found. Fallback to oracle reward function.")
# line 208
			self.rewards = self.spec_r.compute_reward(self) if self.reward_ok == True else self.compute_reward_oracle()
```

2. The RL training script accepts the following three arguments: 

```python
parser.add_argument('--episode_num', type=int, default=480) # termination condition
parser.add_argument('--iter', type=int, default=1, help='iter-th iterations')
parser.add_argument('--reward_no', type=int, default=1, help='reward choice index')
```

3. Generate numerical training logs by following the provided training log template. The file should be named `f'{log_prefix}_ITER{args.iter}_REWARD{args.reward_no}.txt'` and stored in the location specified by `log_dir` in `config.py`.

## Combine with Human Prior Knowledge

ERFSL can operate in a zero-shot manner without direct human feedback, but it can also be combined effectively with human knowledge. Here are a few simple ways to combine the Reward Weight Searcher process with human knowledge.

### Use ERFSL to Adjust the Weights of Human-Designed Reward Functions

Simply specify the reward function to be adjusted in the path `reward_funcs/`. Specifically, the code file whose weights need to be adjusted should be named `reward_code_fin.py`. Ensure that the weight factor you need to adjust clearly corresponds to requirements, and that it is separate from the function code, for example:

```python
​```python
def compute_reward(self):
    # ------ PARAMETERS  ------
    # weights output independently
    w_a; # requirement A
    r_b; # requirement B
    # -------- Code ----------
    # Reward function core python code to be written
    reward += w_a * ... # Utilizing weight variables to calculate reward.
​```
```

To facilitate text matching, copy the delimiters `# ------ PARAMETERS ------` and `# -------- Code ----------` directly.

### Feedback with Human Preference

If you cannot provide explicit numerical objectives for some requirements or need to intervene in the parameter search process, you can specify these two arguments to allow the reward search script to receive human feedback:

```bash
python reward_weight_search.py --human_mod --time_limit 300
```

If these parameters are specified, the system notification is triggered once after the completion of each training iteration and waits for 300 seconds for a user response (5 minutes, as specified by the command-line parameter '--time_limit'. A value of 0 indicates an infinite waiting time). When the user responds, the script outputs performance metrics and waits indefinitely for the user to input feedback in natural language format. Providing better feedback can guide targeted and diverse adjustments for the LLM, but minimal feedback will not impede the search process.

> (A full example) I believe that requirement A is over-optimized and should be weighted less and should take larger step sizes. At the same time, you can try to increase the weight of requirement B, or increase the weight of C, or both.
>
> (Simpler one) Requirement A is still slightly unmet, and I expect its performance indicator to eventually reach 50.

If the user does not respond within a `--time_limit` timeout or responds but abandons giving feedback, the parameters are adjusted automatically, and the next iteration of training is performed without any user feedback.